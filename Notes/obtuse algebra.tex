\documentclass[12pt]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz, tikz-cd}
\usepackage[shortlabels]{enumitem}

\usepackage{mathrsfs}

% Problem Box
\setlength{\fboxsep}{4pt}
\newlength{\myparskip}
\setlength{\myparskip}{\parskip}
\newsavebox{\savefullbox}
\newenvironment{fullbox}{\begin{lrbox}{\savefullbox}\begin{minipage}{\dimexpr\textwidth-2\fboxsep\relax}\setlength{\parskip}{\myparskip}}{\end{minipage}\end{lrbox}\framebox[\textwidth]{\usebox{\savefullbox}}}

% Environments
\setlist[enumerate]{nosep}
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\sepline}{\rule{\textwidth}{0.4pt}}

% Tikz Environments
\newenvironment{drawing}{\begin{center}\begin{tikzpicture}}{\end{tikzpicture}\end{center}}
% \tikzcdset{row sep/normal=0pt}
\newenvironment{cd}{\begin{center}\begin{tikzcd}}{\end{tikzcd}\end{center}}


% Document Formatting
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

% Math Formatting
\newcommand{\ds}{\displaystyle}
\newcommand{\isp}[1]{\quad\text{#1}\quad}
\newcommand{\tc}[1]{, \qquad \text{#1}}
\newcommand{\mc}[1]{, \qquad #1}
\newcommand{\cfa}[1]{, \qquad \text{for all $#1$}}

% mathbb
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}


% mathcal
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\ZZ}{\mathcal{Z}}
\newcommand{\UU}{\mathcal{U}}

% mathfrak
\newcommand{\mm}{\mathfrak{m}}

% symbols
\newcommand{\eps}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\emptyset}{\varnothing}

% Delimiters
\newcommand{\<}{\left\langle}
\renewcommand{\>}{\right\rangle}

% Relations
\newcommand{\iso}{\cong}
\newcommand{\seq}{\subseteq}
\newcommand{\teq}{\trianglelefteq}
\newcommand{\tensor}{\otimes}

\newcommand{\inc}{\hookrightarrow}
\newcommand{\To}{\longrightarrow}
\newcommand{\Mapsto}{\longmapsto}

\newcommand{\eqby}[1]{\overset{\mathrm{(#1)}}{=}}

% Operators


% Math Roman
\newcommand{\op}{\mathrm{op}}
\newcommand{\id}{\mathrm{id}}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\codom}{codom}
\DeclareMathOperator{\supp}{supp}

\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\Ann}{Ann}

% Other
\newcommand{\eqc}{\overline}
\newcommand{\udl}{\underline}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}

\renewcommand{\_}[1]{{_{#1}}}


\title{Algebra \\
    \large 
}
\author{}
\date{}


\begin{document}

We assume some amount of set theory.

ordered tuples and finite cartesian product

\sepline

The definitions given here are not good for intuition.
The main purpose is to provide (robust?) set-theoretic \emph{implementations} of familiar algebraic constructions.
This is a key distinction to be made---I am essentially assuming the reader is familiar with most if not all of the concepts and is able to recognize that the definitions (stroke implementations) adequately capture the necessary functionality.

A mathematical concept in the mind is characterized by some \emph{information}---that being the information necessary to understand the concept.
In my usage ``information'' does not exist in the mathematical world, but rather it exists in each individual mathematicians mind.
We hope that this information generally agrees across minds, and in such cases we consider the information characterizing a mathematical object to be the information which is common in the minds of the mathematical community at large.
It is this standard of information to which we will hold definitions.

In order to capture the information of a mathematical concept, we will use \emph{data}---something which does exist in the mathematical world.
Again, I have my own usage of the word ``data,'' which is luckily more concrete than that of ``information.''
My usage will hopefully become more clear as you read on, but suffice it to say that data is a specific mathematical construction intended to represent the information of some mathematical concept.

To be a bit more opaque, I will employ an analogy and some precise yet non-mathematical terminology.
We can consider mathematical concepts like fictional characters.
When we gives names to these concepts, there is no actual referent (though Salmon and I would prefer designatum, here) to which the name applies (refers/designates you get the idea).
(Unfortunately, my choice of the word ``concept'' for a mathematical audience now conflicts with the point I am making because we can of course refer to concepts, as I have been doing.)
If you're willing to take my word for it---which you absolutely should not---the sort thing I have been calling a ``concept'' is better described in other terms, namely ``sense.''
For normal sorts of things, you can think of the sense of a name as the tether between the name itself and the referent.
The name (word) ``goose'' has some sense attached to it which allows us to pick out the actual animal, i.e., the goose.
One can think of the sense of the word ``goose'' as that thing which allows you to read and understand that it refers to the goose.

Anyway, definitions are gonna look like this:

\begin{fullbox}
    A \keyword{whimple} $W$ is given by the following data:
    \begin{enumerate}[(1)]
        \item an aggle-poogle $A$,
        \item a buzzo $B$ with impermeable flingos,
        \item a crundo $C$ (called the \emph{crun} inside $W$),
    \end{enumerate}
    such that
    \begin{enumerate}[(i)]
        \item $A$ and $B$ are equivalent as aggle-poogles,
        \item each dellon of $C$ is also a dellon of $A$.
    \end{enumerate}
    Say $W$ is a whimple or $W = (A, B, C)$ is a whimple.
\end{fullbox}

In other words, definitions of mathematical objects will be given as set-theoretic implementations in terms of collections of data satisfying certain axioms.
Then to name such an object, we write it as the ordered tuple of data.

\sepline

If $A$ and $B$ are sets, a subset $F \seq A \times B$ is called \keyword{functional} if for each $a \in A$ there is exactly one $b \in B$ such that $(a, b) \in F$.

A \keyword{function} $f$ is given by the following data:
\begin{enumerate}[(1)]
    \item a set $A$ called the \emph{domain} of $f$ (denoted ``$\dom f$''),
    \item a set $B$ called the \emph{codomain} of $f$ (denoted ``$\codom f$''),
    \item a functional subset $F \seq A \times B$ called the \emph{graph} of $f$.
\end{enumerate}
We say that $f$ is a function \emph{from $A$ to $B$}, denoted $f : A \to B$ or $A \xrightarrow{f} B$.

Given $a \in A$, the unique element $b \in B$ such that $(a, b) \in F$ is called the \keyword{value} of $f$ at $a$, often denoted by $f(a)$, though sometimes denoted by $fa$ or $f_a$.
This is can also be called the evaluation of $f$ at $a$ or the image of $a$ under $f$.

This definition adapted from [Ronald Brown, TOPOLOGY AND
GROUPOIDS: A Geometric Account of General Topology,
Homotopy Types and the Fundamental Groupoid]

\sepline

We make a brief distinction here between our specific \emph{definition} of the word ``function'' and the (more general) conceptual \emph{notion} of a function.
It is impossible to say in words precisely what the latter is, but we can make reference to it by the role it serves in the mathematical world.
At one point or another---hopefully rather early in their career---a mathematician will acquire the correct notion of a function.
The sense in which the notion is \emph{correct} is that it is consistent with the notions of the vast majority of mathematicians.


We might ask the mathematician, ``What is a function?''
To which they might respond, ``A function from $A$ to $B$ is a rule which assigns, to each element of $A$, an element of $B$.''
You---the reader---can now act as an impartial third party to me---the author---and this other mathematician I have manufactured.
With any luck, you are able to read and understand both this definition and my own definition of the word ``function.''
Moreover, I hope you agree that they both refer to the same \emph{thing}---that being the notion of a function.


The goal of the above definition is that any sufficiently experienced mathematician will agree that I am using the word ``function'' consistently with broader mathematics, even if some of the specifics are different.
In other words, my definition is attempting to capture the notion of a function by the set-theoretic data which characterizes it.


To be explicit, we are not defining functions per se, rather we are setting up a correspondence between functions $f : A \to B$ and triples of data $(A, B, F)$ satisfying our definition of function.

In a completely set theoretic construction, we would identify every function $f$ with its corresponding triple of data, i.e., $f = (A, B, F)$.
In which case, we are essentially claiming that there is a bijection between functions in the usual sense and their corresponding triples.

$g = (C, D, G)$ is a 


The justification for this sort of definition is that it ensures two functions $f : A \to B$ and $f' : A' \to B'$ are equal

\sepline

The domain of $f$ can be recovered from its graph $F$ as follows:
\[
    \dom f = \{a \mid (a, b) \in F \text{ for some } b\} = A.
\]
On the other hand, the codomain of $f$ cannot be recovered in this way.
Instead, we find the following subset of the codomain called the \keyword{image} of $f$:
\[
    \im f := \{b \mid (a, b) \in F \text{ for some } a\} \seq B.
\]
The domain of $f$ and the evaluation can also be used to find the image as follows:
\[
    \im f = \{f(a) \mid a \in A\} \seq B.
\]
Lastly, the graph of $f$ can be recovered from the domain and the evaluations as follows:
\[
    \Gamma_f := \{(a, f(a)) \mid a \in A\} = F.
\]



If desired, $B^A$ is used to denoted the set of all functions from $A$ to $B$.



\sepline

Let $S$ be a set.
We define the following functions:

the \keyword{identity} function $\id_S : S \to S$ where $a \mapsto a$,

the \keyword{diagonal} function $\Delta_S : S \to S \times S$ where $a \mapsto (a, a)$,

the unnamed (component interchange) function $\lambda_S : S \times S \to S \times S$ where $(a, b) \mapsto (b, a)$.




\newpage

Let $S$ be a (nonempty?) set and $n$ be a nonnegative integer.

The \keyword{$n$th cartesian power} of $S$ is the set of all $n$-tuples of elements of $S$, denoted
\[
    S^n
        = \underbrace{S \times S \times \cdots \times S}_{n \text{ times}}
        = \{(a_1, \dots, a_n) \mid a_i \in S\}.
\]
By convention, we take $S^0 = \{()\} = \{\emptyset\}$ (the set containing only the empty tuple/set).

An \keyword{$n$-ary operation} on $S$ is a function $S^{n} \to S$.

Notably, a $0$-ary (nullary) operation is a function $f : \{\emptyset\} \to S$.
In other words, $f$ picks out a single element of $S$, namely $f(\emptyset) \in S$.
In fact, there is a bijective correspondence between the elements of $S$ and the nullary operations on $S$, given by
\begin{align*}
    S &\longleftrightarrow \{\text{nullary operations on } S\}, \\
    a &\longleftrightarrow (\emptyset \mapsto a).
\end{align*}
Under this correspondence, we can consider distinguished elements of a set to be nullary operations, and vice versa.
This is not a super important point, but simply makes the following definitions more concise, maybe.
Is it worth it? Who is to say?

\sepline

We want more than static sets---we want to be able to do stuff with the sets.
Similar to our definition of ``function,'' we want to define the term ``algebraic structure.''
Put in an unhelpfully circular way, an algebraic structure is the sort of mathematical construction inside of which we can perform algebraic activities.
In the fundamental cases, it consists of a set and some rules for how we can manipulate the elements of the set, e.g., operations.
This paragraph is devoid of meaning.
I am going to provide a set-theoretic implementation now.


An \keyword{algebraic structure} $A$ (simple/on a set?) is given by the following data:
\begin{enumerate}
    \item a set $S$, called the \emph{underlying set} of $A$ (denoted ``$|A|$''),
    \item some operations $\alpha, \beta, \dots$ of any arity on $S$.
\end{enumerate}
We say $A = (S, \alpha, \beta, \dots)$ is an algebraic structure.

There will usually only be finitely many operations on $S$ and in any case there isn't, we will use more compact notation.

\sepline



A \keyword{magma} $M$ is given by the following data:
\begin{enumerate}[$\bullet$]
    \item a set $S$,
    \item a binary operation $f : S \times S \to S$.
\end{enumerate}
We say $M = (S, f)$ is a magma.

A magma $M$ is (naturally) an algebraic structure $(S, f)$.

It is common convention to notate the binary operation with a symbol called a (binary) \emph{operator}---common examples include the following:
\[
    + \quad \times \quad \cdot \quad * \quad \star \quad \circ
\]
This is of course not a comprehensive catalog, though it covers most of the basic cases.
If we choose the star `$\star$' to represent our binary operation, we write `$a \star b$' to mean the image in $S$ under $f$ of the pair $(a, b) \in S \times S$.
In other words, $f$ describes a rule which takes two elements $a, b \in A$ and produces a third element of $S$, denoted 
\[
    a \star b = f(a, b).
\]
This new element $a \star b \in S$ might be called many different things depending on the actual context.

We will say things like ``$S$ is a set with a binary operation $\star$'' or ``$\star$ is a binary operation on the set $S$.''
For brevity, we will also write ``$(S, \star)$ is a magma'' or possibly ``$(S, f)$ is a magma'' if we want to emphasize the fact that the binary operation is a function $S \times S \to S$.

In the broader mathematical world, it is common to consider only a single binary operation on a set at one time---there are many sets with `canonical' operations---which can lead to the set alone being taken as proxy for the magma.
For example, to refer to what we call ``the magma $(S, \star)$,'' one might instead say ``the set $S$ \textit{equipped with} a binary operation $\star$.''
The difference here is subtle and a passing glance is not likely to reveal a discrepancy beyond the obvious linguistic one.
And indeed there is no difference to one interfacing with mathematics in its most pure conceptual form.
But you and I---dear reader---are confined to the relentlessly imperfect and finite bounds of language.
When we do mathematics, we must undertake the impossible challenge of justifying that the sense connecting our mathematical referents to our mathematical language is justified, but can only do so either in the very same mathematical language or in some natural language.
(The latter is what a mathematician might call `intuition' (for the more cultured `being loosey goosey' or `feeding the geese')).

I trailed off, but the minor point is that it is unreasonable to be fully explicit all of the time.
And while there are sometimes good reasons to `compress' mathematical data into containers not designed for it, there should be a way to unpack that data when necessary (in my opinion of course).
I would also say that neither formality nor explication exist solely on single axis of `less' or `more,' where one must simply pick a single point along that axis.
A thinner dictionary can be seen as either `clean and elegant' or `intuitive and superficial,' while a thicker one as either `rigorous and complete' or `cluttered and illegible' (find more creative or meaningful adjectives).
A proof on either end of this can also be more or less transparent/opaque/illuminating/aesthetic/motivating.
(Hot take things have many aspects and rarely does a single aspect determine its quality.)

We are rarely interested in a general binary operation and will most often require it to have some additional structure.
In such cases, however, we will need to define a particular binary operation and prove that it has the desired structure.
For this reason---I would argue---it is worth having the language to talk about general binary operations.

(There is a diversion here where I lament about having to prove certain elementary properties of objects without having the proper language to even talk about the way in which those objects possess those properties.
To make matters worse, it is often the case that the desired result is essentially some form of ``niceness'' in the sense that we are showing that some hypothetical bad situation never occurs. In which case, when we are using the object for its intended purpose, we can sweep certain technical nuances under the rug.
So because the goal is to be able to ignore the nuance, one has to synthesize a complete model of this nuance only to discard it the moment it reveals its own unimportance.)

\sepline

An \keyword{algebraic structure} $A$ is given by (some or all of) the following data:
\begin{enumerate}[(1)]
    \item an \emph{underlying} algebraic structure, denoted explicitly by $\udl{A}$ unless otherwise specified,
    \item some \emph{auxiliary} algebraic structures/objects,
    \item some homomorphisms between these algebraic structures.
\end{enumerate}

An algebraic structure \keyword{class/type} is a \emph{signature} of data for constructing an algebraic structure.
The signature prescribes the following: whether or not and from which category an underlying object is required, whether or not and from which category any auxiliary objects are required, whether or not and from which category any morphisms are required.
Additionally, the signature prescribes some \emph{coherence conditions} which must be satisfied by the supplied data.

In other words, an algebraic structure class/type describes a format/template for constructing a more narrow sort of object.
Any algebraic object which conforms to the template, is said to be of the class/type in question.

Whenever an algebraic object $A$ of type $\mathcal{A}$ has an underlying structure $\udl{A}$, then we very closely identify 

hey bud these are just categories. just use categories

\sepline

A \keyword{magma} $M$ is a type of algebraic structure given by the following data:
\begin{enumerate}[(1)]
    \item an underlying set $\udl{M}$,
    \item a binary operation on $\udl{M}$ (a function $\udl{M} \times \udl{M} \to \udl{M}$).
\end{enumerate}

\sepline

Let $M = (\udl{M}, \odot)$ and $N = (\udl{N}, \otimes)$ be magmas.

\sepline

In order to notate an expression containing multiple applications of the operation, we use parenthesis, e.g.,
\[
    a \star (b \star c) = f(a, f(b, c)) \isp{and} (a \star b) \star c = f(f(a, b), c).
\]
In general, it is not well-defined to write an expression like `$a \star b \star c$,' as it is possible that the result is dependent on the order in which we apply the operation.
In cases where the this order does not matter, we can make sense of such notation.

A binary operation $\star$ on a set $S$ is called \keyword{associative} if
for all $a, b, c \in S$ we have
\[
    (a \star b) \star c = a \star (b \star c)
\]

A magma $(S, \star)$
\begin{itemize}
    \item (ass) is \keyword{associative} if $(a \star b) \star c = a \star (b \star c)$ for all $a, b, c \in S$,
    \item (id) has \keyword{identity} if there exists $e \in S$ such that $e \star a = a \star e = a$ for all $a \in S$,
    \item (inv) has \keyword{inverses} if for every $a \in S$ there exists $b \in S$ such that $ab = ba = e$,
    \item (comm) is \keyword{commutative} if $a \star b = b \star a$ for all $a, b \in S$.
\end{itemize}

A magma $(S, \star)$ satisfying certain properties typically has a more specific name.
We say $(S, \star)$ is a
\begin{itemize}
    \item \keyword{semigroup} if (ass) it is associative,
    \item \keyword{monoid} if (ass, id) it is associative and has an identity,
    \item \keyword{group} if (ass, id, inv) it is associative, has an identity, and has inverses,
\end{itemize}
Most of these will simply take ``commutative'' as an adjective, though we usually say \keyword{abelian group} to mean a commutative group, i.e.,
\begin{itemize}
    \item \keyword{abelian group} if (ass, id, inv, comm) it is associative, has an identity, has inverses, and is commutative.
\end{itemize}

Chart:
\begin{center}
    \begin{tabular}{r|cccc}
         & ass & id & inv & comm \\
            \hline
        magma & - & - & - & - \\
        semigroup & x & - & - & - \\
        monoid & x & x & - & - \\
        comm.\ monoid & x & x & - & x \\
        group & x & x & x & - \\
        abelian group & x & x & x & x \\
    \end{tabular}
\end{center}

semigroup bad, just add identity to get monoid.

``you can define lots of things, but that doesn't mean you should study them.'' - Dave Morrison

\sepline

A \keyword{monoid} is given by the following data:
\begin{enumerate}[(1)]
    \item s set $S$,
    \item a distinguished element $e \in S$,
    \item a binary operation $\star$ on $S$,
\end{enumerate}
such that
\begin{enumerate}[(i)]
    \item (ass) $(a \star b) \star c = a \star (b \star c)$ for all $a, b, c \in S$,
    \item (id) $e \star a = a \star e = a$ for all $a \in S$.
\end{enumerate}
In which case, we say $(S, e, \star)$ is a \keyword{monoid}.

\sepline

Let $(S, e, \cdot)$ be a monoid.
(For simplicity, write $ab = a \cdot b$ for all $a, b \in S$.)

Given $a, b \in S$ such that $ab = e$, we say that $a$ is a \keyword{left inverse} of $b$ and $b$ is a \keyword{right inverse} of $a$.
If in addition $ba = e$, then we say that $a$ is an \keyword{inverse} of $b$, and vice versa.
Equivalently, we might say
\begin{itemize}[nosep]
    \item $a$ and $b$ are inverses (of each other),
    \item $a$ is inverse to $b$,
    \item $a$ and $b$ are inverse (to one another).
\end{itemize}
Additionally, we say that $a$ and $b$ are \keyword{invertible} in the monoid.

(One must be careful when using the word ``invertible,'' as it is relative to both the underlying set and the operation---e.g., every element of $\Z$ is invertible with respect to addition, though only $1$ and $-1$ are invertible with respect to multiplication.)

Note $a$ and $b$ are inverses if and only if one is both a left and right inverse of the other.

In general mathematics, it is more common to hear the phrase ``$a$ is \textit{the} inverse of $b$'' rather than ``$a$ is \textit{an} inverse of $b$.''
Indeed, in most nice cases, there is only one inverse.
In turns out that this uniqueness holds in any monoid, as we will now demonstrate.

\begin{proposition}
    If $a, b, c \in S$ are such that $ab = ca = e$, then $b = c$.
\end{proposition}

\begin{proof}
    $b \eqby{id} eb = (ca)b \eqby{ass} c(ab) = ce \eqby{id} c$.
\end{proof}

In particular, the common value of $b$ and $c$ is an inverse of $a$.
It follows that an element of a monoid is invertible whenever it has both a left and a right inverse---the proposition implies that the two are equal.

\begin{corollary}
    Inverses are unique.
\end{corollary}

\begin{proof}
    If $b$ and $c$ are both inverses of $a$, the proposition implies $b = c$.
\end{proof}

Hence, every invertible element of the monoid in fact has a unique inverse.
If $a \in S$ is invertible, then we denote \keyword{the inverse} of $a$ might be denoted by one of the following
\[
    -a \qquad a^{-1} \qquad \frac{1}{a} \qquad \overline{a} \qquad a'
\]

\sepline

Let $(G, e, \cdot)$ be a group, i.e., a monoid with all elements invertible.
Since the inverses in a monoid are unique, so too are the inverses in a group.
And since every element has an inverse, there is a well-defined function 
\begin{align*}
    \iota : G &\To G, \\
        g &\Mapsto g^{-1},
\end{align*}
where $g^{-1}$ is \textit{the} inverse of $g$ in the group.
This function has the following properties:
\begin{enumerate}[(i)]
    \item $\iota \circ \iota = \id_G$
    \item $\iota(g \cdot h) = \iota(h) \cdot \iota(g)$, i.e., $(gh)^{-1} = h^{-1}g^{-1}$
\end{enumerate}

\sepline

Let $G = (S, e, \cdot)$ be a group.
We can define a new binary operation $\odot$ on $S$ as follows:
\[
    g \odot h := h \cdot g \isp{or} \mu_\odot = \mu_\cdot \circ \lambda_G.
\]
Denote the algebraic structure $G^\op = (S, e, \odot)$.
We claim $G^\op$ is a group.
Indeed $(S, \odot)$ is a magma and $e \in S$ is a distinguished element, so we have the required data. (unless i later require the inversion map for groups)
We check the axioms now:
\begin{itemize}[nosep]
    \item (ass) $(g \odot h) \odot k = k \cdot (h \cdot g) = (k \cdot h) \cdot g = g \odot (h \odot k)$,
    \item (id) $g \odot e = e \cdot g = g = g \cdot e = e \odot g$,
    \item (inv) $g \odot g^{-1} = g^{-1} \cdot g = e = g \cdot g^{-1} = g^{-1} \odot g = e$, where $g^{-1}$ is the inverse of $g$ in $G$. 
\end{itemize}
Hence, $G^\op$ is a group, called the \emph{opposite group} of $G$.

If $\iota : S \to S$ is the inversion function of $G$, then we have
\[
    \iota(g \cdot h) = \iota(h) \cdot \iota(g) = \iota(g) \odot \iota(h).
\]
That is, $\iota$ specifies a group homomorphism (what's that bud?) $G \to G^\op$.
In fact, this is an isomorphism and $(G^\op)^\op = G$.


\sepline

Let $X$ and $Y$ be sets.

If there is a suitable `zero' element $0 \in Y$, then the \keyword{support} of a function $f : X \to Y$ is the set
\[
    \operatorname{supp} f
        = \operatorname{supp}(f)
        := \{x \in X \mid f(x) \ne 0\}
        = f^{-1}(Y \setminus \{0\})
        = X \setminus f^{-1}(0).
\]

As a pseudo-definition, say $f$ has \keyword{finite support} when $\supp(f)$ is a finite set.

Recall that the support of a partial function is the subset of its domain on which it is defined.

Then $f$ induces a partial function $\tilde{f}$ from $X$ to $Y \setminus \{0\}$.
Then the support of $f$ in the above sense is the same as the support of $\tilde{f}$ as a partial function.

The upshot is that the ``support'' of a function usually refers to the subset of the domain whose values we actually care about.
In algebraic contexts, this is almost always used with respect to a zero element.

Recall that the set of functions from $X$ to $Y$ is denoted by $Y^X$.

When $Y$ is a suitable codomain, we denote by $Y^{(X)}$ the set of (partial) functions $X \to Y$ with finite support.


\sepline

To be extremely pedantic, let
\[
    \ZZ = \{\dots, -3, -2, -1, 0, 1, 2, 3, \dots\}
\]
denote the set of integers.
I emphasize that this is a set with no additional structure.
It doesn't matter which specific set-theoretic structure it has---choose your favorite.

We define a binary operation $+$ on $\ZZ$ such that $a + b$ is `the sum of $a$ and $b$,' for all $a, b \in \ZZ$.
This seems a little circular, and could definitely be formalized better.
We define the abelian group $\Z = (\ZZ, 0, +)$, called the additive group of integers.

Typically, the term ``addition'' is reserved for any operation on a set that is sufficiently similar to the usual addition of integers.
In particular, $\Z$ addition is an abelian group.
In fact, there is a sense in which addition over the integers is the most fundamental nontrivial abelian group.



\sepline

Let $X$ be a set.

A \keyword{formal sum} in $X$ is given by the data of a function $c : X \to \Z$, written as
\[
    c = \sum_{x \in X} c_x \cdot x.
\]
The value $c_x = c(x) \in \Z$ is called the \keyword{coefficient} of $x$.
When we interpret $c$ as a function $X \to \Z$, it is called the \emph{coefficient function}.
The notation is meant to suggest multiplying the coefficient $c(x) \in \Z$ with the element $x \in X$, and taking the sum over all such products.

Since $\Z$ is an abelian group under addition, the set of functions $\Z^X$ is also an abelian group under componentwise addition:
\[
    (a + b)(x) = a(x) + b(x).
\]
Equivalently, this gives us an addition on the set of formal sums, with
\[
    a + b
        = \sum_{x \in X} a_x \cdot x + \sum_{x \in X} b_x \cdot x
        = \sum_{x \in X} (a + b)_x \cdot x
        = \sum_{x \in X} (a_x + b_x) \cdot x.
\]
For each $a \in X$, there is a characteristic function $\chi_a : X \to \Z$ defined by
\[
    \chi_a(x) = \begin{cases}
        1 & x = a, \\
        0 & x \ne a.
    \end{cases}
\]

A \keyword{finite formal sum} in $X$ is a formal sum $\sum_{x \in X} c_x x$ with only finitely many nonzero coefficients, i.e., $c_x \ne 0$ for finitely many $x \in X$.
(function $c : X \to \Z$ has finite support.)

The set of finite formal sums in $X$ may be denoted as any of the following:
\[\arraycolsep=1em
    \begin{array}{cccc}
        \Z \cdot X & \Z X & \Z[X] & \Z^{(X)}
    \end{array}
\]



\sepline

A \keyword{formal sum} in $S$ is a symbolic object which suggests some form of addition over the elements of $S$, but no specific operation is present.
Intuitively, we want to build notation for `the most general sort of addition' over the elements of $S$.



The formal sum of two elements $a, b \in S$ is written as ``$a + b$'' or ``$b + a$''.
Ideally, we would want to interpret these expressions as the same thing.
In other words, we are imagining the operation to be commutative.

The formal sum of three elements $a, b, c \in S$ is written as ``$a + b + c$'' (or any permutation of the order).
With this notation---in particular, not putting parentheses---we are imagining the operation to be associative.


\sepline

Let $A = (\AA, 0, +)$ be an abelian group.




\newpage
\sepline

A \keyword{ring} $R$ is an algebraic structure with the following data:
\begin{enumerate}[(1)]
    \item distinguished elements $0, 1 \in |R|$,
    \item a binary operation $+$ called \emph{addition} such that $(|R|, 0, +)$ is an abelian group,
    \item a binary operation $\cdot$ called \emph{multiplication} such that $(|R|, 1, \cdot)$ is a monoid,
\end{enumerate}
such that the two operations satisfy the following distributivity properties:
\begin{enumerate}[(i)]
    \item (dist) $a \cdot (b + c) = (a \cdot b) + (a \cdot c)$,
    \item (dist) $(a + b) \cdot c = (a \cdot c) + (b \cdot c)$.
\end{enumerate}

By convention, the \emph{order of operations} in a ring puts multiplication before addition.
Because of this, we can omit parentheses the order of operations can be deduced.
Moreover, we typically write the multiplication without the operator, though sometimes use it for clarity.
On the other hand, we always write the operator for addition.
For example, we can write the following
\[
    (a \cdot b) + (c \cdot d) = ab + cd.
\]
The additive inverse of $a \in R$ is denoted by $-a$, and we replace the plus sign with a minus sign when adding an inverse, e.g.,
\[
    a + (-b) = a - b.
\]
The multiplicative inverse of $a \in R$, when it exists, is denoted by $a^{-1}$.



\begin{enumerate}[(i)]
    \item $\mu(a, \alpha(b, c)) = \alpha(\mu(a, b), \mu(a, c))$, i.e., 
    \begin{cd}[column sep=large]
        R \times R \times R \ar[rr, "\Delta_R \times \id_R \times \id_R"] \dar["\id_R \times \alpha"]
            && R \times R \times R \times R \ar[rr, "\id_R \times \lambda_R \times \id_R"]
            && R \times R \times R \times R \dar["\mu \times \mu"]\\
        R \times R \ar[rr, "\mu"]
            && R
            && R \times R \ar[ll, "\alpha"']
    \end{cd}
    \begin{cd}
        R^3 \ar[rr, "\Delta \times \id \times \id"] \dar["\id \times \alpha"']
            && R^4 \ar[rr, "\id \times \lambda \times \id"]
            && R^4 \dar["\mu \times \mu"]\\
        R^2 \ar[rr, "\mu"']
            && R
            && R^2 \ar[ll, "\alpha"]
    \end{cd}
    \item $\mu(\alpha(a, b), c) = \alpha(\mu(a, c), \mu(b, c))$, i.e.,
    \begin{cd}
        R^3 \ar[rr, "\id \times \id \times \Delta"] \dar["\alpha \times \id"']
            && R^4 \ar[rr, "\id \times \lambda \times \id"]
            && R^4 \dar["\mu \times \mu"]\\
        R^2 \ar[rr, "\mu"']
            && R
            && R^2 \ar[ll, "\alpha"]
    \end{cd}
\end{enumerate}

\newpage
\sepline

A \keyword{ring} is given by the following data:
\begin{enumerate}[(1)]
    \item a set $R$;
    \item a binary operation $+$ on $R$ called \emph{addition};
    \item a binary operation $\cdot$ on $R$ called \emph{multiplication};
\end{enumerate}
such that
\begin{enumerate}[(i)]
    \item $R$ is an abelian group under addition:
    \begin{enumerate}
        \item (associativity) $(a + b) + c = a + (b + c)$,
        \item (identity) $0 \in R$ called \emph{zero} such that $0 + a = a + 0 = a$,
        \item (invertibility) for all $a \in R$ there exists an $-a \in R$ such that $a + (-a) = 0$,
        \item (commutativity) $a + b = b + a$;
    \end{enumerate}
    \item $R$ is a monoid under multiplication:
    \begin{enumerate}
        \item (associativity) $(a \cdot b) \cdot c = a \cdot (b \cdot c)$,
        \item (identity) $1 \in R$ called \emph{one} such that $a \cdot 1 = 1 \cdot a = a$;
    \end{enumerate}
    \item multiplication is distributive over addition:
    \begin{enumerate}
        \item (left distributivity) $a \cdot (b + c) = (a \cdot b) + (a \cdot c)$,
        \item (right distributivity) $(a + b) \cdot c = (a \cdot c) + (b \cdot c)$.
    \end{enumerate}
\end{enumerate}

\sepline


\begin{proposition}
    In a ring $R$ we have the following properties:
    \begin{enumerate}[(a)]
        \item $0 \cdot a = a \cdot 0 = 0$,
        \item $(-1) \cdot a = a \cdot (-1) = -a$,
        \item $(-a) \cdot b = a \cdot (-b) = -(a \cdot b)$,
        \item $(-1) \cdot (-1) = 1$,
        \item $(-a) \cdot (-b) = ab$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    beebop
    \begin{enumerate}[(a)]
        \item $0a \eqby{+ id} (0 + 0)a \eqby{dist} 0a + 0a$ implies $0 = 0a$ by cancellation. \\
        $a0 \eqby{+ id} a(0 + 0) \eqby{dist} a0 + a0$ implies $0 = a0$ by cancellation.
        \item $a + (-1)a \eqby{\cdot id} 1a + (-1)a \eqby{dist} (1 - 1)a \eqby{+inv} 0a \eqby{a} 0$ implies $(-1)a = -a$ by additive inverse uniqueness. \\
        $a + a(-1) \eqby{\cdot id} a1 + a(-1) \eqby{dist} a(1 - 1) \eqby{+inv} a0 \eqby{a} 0$ implies $a(-1) = -a$ by additive inverse uniqueness.
        \item $-(ab) \eqby{b} (-1)ab \eqby{b} (-a)b \eqby{b} a(-1)b \eqby{b} a(-b)$.
        \item $(-1)(-1) \eqby{b} -(-1)$ but $-(-1) = 1$ by additive inverse uniqueness.
        \item $(-a)(-b) \eqby{b} a(-1)(-1)b \eqby{d} a1b \eqby{\cdot id} ab$.
    \end{enumerate}
\end{proof}

\sepline

A ring $R$ is \keyword{local} if any of the following equivalent conditions are satisfied:
\begin{itemize}[nosep]
    \item $R$ has a unique maximal left ideal;
    \item $R$ has a unique maximal right ideal;
    \item For all $a, b \in R$, if $a + b$ is a unit then either $a$ or $b$ is a unit.
\end{itemize}

\sepline

A \keyword{}{module} (over a commutative ring) is given by the following data:
\begin{enumerate}[(1)]
    \item a commutative ring $R$,
    \item an abelian group $M$,
    \item a function $\mu : R \times M \to M$ such that
    \begin{enumerate}[(i)]
        \item $\mu(1, x) = x$ or equivalently $\mu(1, -) = \mu|_{1 \times M} = \id_M$
        \item $\mu(ab, x) = \mu(a, \mu(b, x))$ or equivalently the following diagram commutes
        \begin{cd}
            R \times R \times M \rar["\id_R \times \mu"] \dar["\mu_R \times \id_M"'] & R \times M \dar["\mu"] \\
            R \times M \rar["\mu"'] & M
        \end{cd}
    \end{enumerate}
\end{enumerate} 

\sepline

Let $R$ be a commutative ring.

An \keyword{$R$-module} is given by the following data:
\begin{enumerate}[(1)]
    \item an abelian group $M$ written additively, i.e., identity is $0$ and binary operation is $+$,
    \item a ring homomorphism $\mu : R \to \End(M)$, called \emph{scalar multiplication}.
    \item a $\Z$-module homomorphism $\mu : R \tensor_\Z M \to M$, called \emph{scalar multiplication}.
\end{enumerate}
We usually write $r \cdot m$ for the evaluation $\mu(r)(m)$.

\sepline

Let $R$ be a ring.

A \keyword{left $R$-module} is given by the following data:
\begin{itemize}
    \item an abelian group $M$, written additively, i.e., identity is $0$ and binary operation is $+$;
    \item a function $\mu : R \times M \to M$ called \emph{scalar multiplication} (write $r \cdot m = \mu(r, m)$);
\end{itemize}
such that
\begin{itemize}
    \item $1 \cdot m = m$ for all $m \in M$;
    \item $r \cdot (m + n) = (r \cdot m) + (r \cdot n)$ for all $r \in R$ and $m, n \in M$;
    \item $(r + s) \cdot m = (r \cdot m) + (s \cdot m)$ for all $r, s \in R$ and $m \in M$;
    \item $r \cdot (s \cdot m) = (rs) \cdot m$ for all $r, s \in R$ and $m \in M$.
\end{itemize}

A \keyword{right $R$-module} is given by the following data:
\begin{itemize}
    \item an abelian group $M$, written additively, i.e., identity is $0$ and binary operation is $+$;
    \item a function $\mu : M \times R \to M$ called \emph{scalar multiplication} (write $m \cdot r = \mu(m, r)$);
\end{itemize}
such that
\begin{itemize}
    \item $m \cdot 1 = m$ for all $m \in M$;
    \item $(m + n) \cdot r = (r \cdot m) + (r \cdot n)$ for all $r \in R$ and $m, n \in M$;
    \item $m \cdot (r + s) = (r \cdot m) + (s \cdot m)$ for all $r, s \in R$ and $m \in M$;
    \item $(m \cdot s) \cdot r = m \cdot (sr)$ for all $r, s \in R$ and $m \in M$.
\end{itemize}

\sepline

Let $R = (\RR, 0, 1, +, \cdot)$ be a ring.

We construct a the \keyword{opposite ring} $R^\op = (\RR, 0, 1, +, \cdot^\op)$, where
\[
    r \cdot^\op s = s \cdot r
\]

\sepline

Let $R$ be a ring and $M$ be an abelian group.

A left $R$-module structure on $M$ is equivalently a ring homomorphism $\lambda : R \to \End(M)$, with
\[
    r \cdot m = \lambda(r)(m)
\]

A right $R$-modules over $M$ is a ring homomorphism $R^\op \to \End(M)$.








\newpage
\sepline

Let $R$ be a ring.

The \keyword{center} of $R$ is the set
\[
    Z(R) = \{r \in R \mid ax = xr \text{ for all } x \in R\}.
\]

$Z(R)$ is a commutative subring of $R$.

$R$ is commutative if and only if $Z(R) = R$.

It is not in general true that $Z(R)$ is a maximal commutative subring of $R$.

\sepline

Let $R$ be a commutative ring.

An \keyword{associative unital $R$-algebra} consists of
\begin{itemize}
    \item A ring $A$;
    \item A ring homomorphism $\phi : R \to Z(A)$.
\end{itemize}
We will consider all algebras to be associative unital until further notice.

If $K$ is a field, a $K$-algebra is equivalently a $K$-vector space $V$ with the structure of a ring such that
\[
    a(xy) = x(ay) = (ax)y
\]
for all $a \in K$ and $x, y \in V$.

Let $(A, \phi)$ and $(B, \psi)$ be $R$-algebras

An \keyword{$R$-algebra homomorphism} is a ring homomorphism $f : A \to B$ such that
\[
    f(\phi(r)a) = \psi(r)f(a)
\]
for all $r \in R$ and $a \in A$.


There is a category $R\text{-}\mathsf{Alg}$ of $R$-algebras and $R$-algebra homomorphisms.

There are forgetful functors
\begin{cd}
    & RAlg 
        \dlar
        \dar
        \drar
    \\
    R/Ring
        \drar
        \rar
    & Ring
        \dar
    & RMod
        \dlar
    \\
    & Set
\end{cd}
\begin{cd}
    & \phi : R \to Z(A)
        \dlar
        \dar
        \drar
    \\
    R \to A
        \drar
        \rar
    & A
        \dar
    & \phi(-) \cdot -
        \dlar
    \\
    & A
\end{cd}

\sepline





\sepline

Let $R$ be a commutative ring.

An \keyword{associative unital $R$-algebra} is equivalently any of the following:
\begin{itemize}
    \item A monoid object of $R$-Mod with respect to the tensor product of $R$-modules;
    \item An $R$-module $V$ with $R$-linear maps $p : V \tensor V \to V$ and $i : R \to V$ satisfying associative and unit laws;
    \item A ring $A$ with a ring homomorphism $R \to A$ whose image is in the center of $A$.
\end{itemize}

\sepline

Let $R$ be a ring and let $I$ and $J$ be sets.

An \keyword{$I \times J$-matrix over $R$} consists of a function $A : I \times J \to R$.
Call $a_{ij} = A(i, j) \in R$ the $(i, j)$ entry of $A$, write
\[
    A = [a_{ij}]_{I \times J}.
\]
For $I' \seq I$ and $J' \seq J$ the restriction $A|_{I' \times J'} = [a_{ij}]_{I' \times J'}$ is called a \keyword{submatrix} of $A$.

For $i \in I$, the restriction $A|_{i \times J} = [a_{ij}]_{i \times J}$ is called a \keyword{row} of $A$.

For $j \in J$ the restriction $A|_{I \times j} = [a_{ij}]_{I \times j}$ is called a \keyword{column} of $A$.

Denote the collection of such matrices by either of the following:
\[
    R^{I \times J} = M_{I \times J}(R).
\]

Something about finiteness.

Get pointwise addition on $R^{I \times J}$, so is abelian group.

For $A \in R^{I \times J}$ and $B \in R^{J \times K}$, there is a function
\begin{align*}
    AB : I \times K &\To \Sigma_J R \\
        (i, k) &\Mapsto \sum_{j \in J} a_{ij}b_{jk}
\end{align*}

If either $A$ is row finite or $B$ is column finite, each formal sum has finitely many nonzero entries, so is a well-defined element of $R$.
In which case, $AB$ is an $I \times K$-matrix.

``It is easy but tedious to show that where this product is defined, it is associative and that it distributes over addition on both the right and left.''

The identity matrix $[\delta_{ij}] \in R^{J \times J} = M_J(R)$ is row and column finite and serves as the identity for the partial binary operation of multiplication.

Both the collections column finite and row finite square matrices are rings.

\sepline

Let $R$ be a ring.

An element $e \in R$ is called an \keyword{idempotent} if $e^2 = e$.
(e.g. $0$ and $1$).

A pair of idempotents $e_1, e_2 \in R$ are \keyword{orthogonal} if $e_1e_2 = e_2e_1 = 0$.

An idempotent $e \in R$ is \keyword{primitive} if $e \ne 0$ and for every pair of orthogonal idempotents $e_1, e_2 \in R$, $e = e_1 + e_2$ implies $e_1 = 0$ or $e_2 = 0$.
($e$ is not the sum of two nonzero orthogonal idempotents.)

A left (right) ideal of $R$ is \keyword{primitive} if it is of the form $Re$ ($eR$) for some primitive idempotent $e \in R$.

\sepline

If $e \in R$ is an idempotent, so is $1 - e$:
\[
    (1 - e)^2
        = 1 - e - e + e^2
        = 1 - e - e + e
        = 1 - e.
\]

Each nonzero idempotent $e \in R$ determines a ring
\[
    eRe = \{ere \mid r \in R\},
\]
whose addition and multiplication are the restriction of that in $R$, and whose identities are $0 = e0e$ and $e = e1e$.
If $e \ne 1$, then $eRe$ is not a subring of $R$ and if $e$ is not central then the natural map $R \to eRe$ need not be a ring homomorphism.

If $e$ is central, then the map $\tau_e : r \mapsto ere$ is a surjective ring homomorphism $R \to eRe$ with kernel $(1 - e)R(1 - e)$.

Let $R = \prod_{A} R_\alpha$ be the cartesian product of rings.
For $\alpha \in A$, the element $e_\alpha = (\delta_{\alpha\beta}^{R_\beta})_{\beta \in A}$ is a central idempotent of $R$ and there is an isomorphism $e_\alpha R e_\alpha \to R_\alpha$ given by the restriction of the projection $\pi_\alpha$.

Let $R$ be a ring and $1 \leq m \leq n$ be integers.
Consider the matrix $e = [a_{ij}]$ defined by
\[
    a_{ij} = \begin{cases}
        1 & \text{if } i = j \leq m, \\
        0 & \text{otherwise}.
    \end{cases}
\]
Put another way,
\[
    e = \mat{I_m & 0 \\ 0 & 0}.
\]
Then $e$ is a nonzero idempotent and there is a ring isomorphism
\[
    eM_n(R)e \iso M_m(R).
\]
The same is true for any $e$ with exactly $m$ nonzero entries, all of which are $1$'s on the diagonal.

\sepline

Let $R$ be a ring.

An element $r \in R$ is called \keyword{nilpotent} if $r^n = 0$ for some $n \in \Z_{>0}$.
The \keyword{nilpotent index/degree} of a nilpotent $r$ is the smallest $n \in \Z_{>0}$ such that $r^n = 0$:
\[
    \min\{n \in \Z_{>0} \mid r^n = 0\}.
\]

A subset $A \seq R$ is \keyword{nilpotent} if there exists $n \in \Z_{>0}$ such that
\[
    a_1a_2 \cdots a_n = 0
\]
for all sequences $a_1, \dots, a_n \in A$.

A subset $A \seq R$ is \keyword{nil} if each of its elements is nilpotent.

\sepline

Remarks/exercises.

$0$ is the only element which is both idempotent and nilpotent.

$M_n(R)$ has many nilpotents.

If $x \in R$ is nilpotent then $1 - x$ is a unit.

Let $\phi : R \to S$ be a surjective ring homomorphism.
If $a \in R$ is a unit, central, idempotent, or nilpotent, respectively, then so is $\phi(a) \in S$.
How about converses?

Let $R$ be a ring and $n \geq 2$ and integer.
For each ideal $I \teq R$ set
\[
    M_n(I) = \{[a_{ij}] \in M_n(R) \mid a_{ij} \in I\}.
\]
The map $I \mapsto M_n(I)$ defines an isomorphism between the lattices of ideals in $R$ and $M_n(R)$.

If $I \teq R$, then has ring isomorphism $M_n(R)/M_n(I) \iso M_n(R/I)$.


Let $R$ be a ring.
There is a unique ring homomorphism $\chi : \Z \to R$.
The kernel of $\chi$ is of the form $n\Z$ for some unique $n \geq 0$; this $n$ is the \keyword{characteristic} of $R$.


Let $p \in \N$ be a prime.
For each $n \in \N$ the ideals of $\Z/p^n\Z$ form a chain and each proper ideal is nilpotent.
Then the product
\[
    R = \prod_{n > 1} \Z/p^n\Z.
\]
has a nil ideal that is not nilpotent.

\sepline

Proper coverage of modules goes here.

left/right modules, bimodules, etc.

\sepline



A module $M$ is called...
\begin{itemize}
    \item \keyword{decomposable} if $M = N \oplus N'$ with $N$ and $N'$ nonzero

    (A \keyword{direct decomposition} of $M$ is any direct sum $M = \bigoplus_{\alpha \in A} M_\alpha$);

    \item \keyword{indecomposable} if $M$ is not decomposable;

    \item \keyword{simple} if it has no nonzero proper submodules, i.e., $N \leq M$ implies $N = 0$ or $N = M$;

    (sometimes ``irreducible'' for rep theory reasons i think. we'll see if that's useful.)
    
    \item \keyword{completely decomposable} if $M = \bigoplus_{\alpha \in A} M_\alpha$ with each $M_\alpha$ indecomposable

    (called an \keyword{indecomposable decomposition});
    
    \item \keyword{semisimple} if $M = \bigoplus_{\alpha \in A} M_\alpha$ with each $M_\alpha$ simple

    (called a \keyword{semisimple decomposition});
\end{itemize}






\sepline

Let $M$ be a left $R$-module.

For $X \seq M$, the \keyword{left annihilator} of $X$ in $R$ is
\[
    \_R\!\Ann(X) = \{r \in R \mid rx = 0 \text{ for all } x \in X\}.
\]
(Notation pending).
For $A \seq R$ the \keyword{right annihilator} of $A$ in $M$ is
\[
    \Ann_M(A) = \{x \in M \mid ax = 0 \text{ for all } a \in A\}.
\]

Let $M$ be an $(R, S)$-bimodule, $X \seq M$ and $A \seq R$.
Then $\_R\!\Ann(X)$ is a left ideal of $R$ and $\Ann_M(A)$ is a submodule of $M_S$.
Moreover if $X \leq \_RM$ then $\_R\!\Ann(X) \teq R$.
If $A$ is a right ideal of $R$ then $\Ann_M(A) \leq \_RM_S$.
If $R$ is commutative, then $\_R\!\Ann(X) \teq R$ and $\Ann_M(A) \leq \_RM_S$.

\sepline

Suppose $\_RM = N \oplus N'$ and $\pi_N : M \to N$ is the natural projection.
Define $e_N \in \End(\_RM)$ by $x \mapsto \pi_N(x)$.
Then $\pi_N|_{N} = \id_N$, $e_N$ is an idempotent endomorphism of $M$, i.e.,
\[
    e_N = e_N^2 \in \End(\_RM)
\]
and $N = Me_N$.

Let $e$ be an idempotent in $\End(\_RM)$.
Then $\id - e$ is an idempotent in $\End(\_RM)$ such that
\[
    \ker e = \im(\id - e) \isp{and} \im e = \ker(\id - e)
\]
and $M = Me \oplus M(\id - e)$.

\sepline

Suppose $M = \bigoplus_{\alpha \in A} M_\alpha$.
Then for each $a \in A$
\[
    M = M_\alpha \oplus \sum_{\beta \ne \alpha} M_\beta,
\]
so by some result, there is a unique idempotent $e_\alpha \in \End(\_RM)$ with
\[
    M_\alpha = \im e_\alpha \isp{and} \ker e_\alpha = \sum_{\beta \ne \alpha} M_\beta.
\]

Moreover the collection of idempotents $\{e_\alpha\}$ is orthogonal and for all $x \in M$ we have $xe_\alpha = 0$ for all but finitely many $\alpha \in A$ with $x = \sum_{\alpha \in A} x e_\alpha$.

\sepline



A finite orthogonal set of idempotents $e_1, \dots, e_n \in R$ is \keyword{complete/full} if $e_1 + \cdots + e_n = 1$.


Let $M_1, \dots, M_n$ be submodules of $M$. Then $M = M_1 \oplus \cdots \oplus M_n$ if and only if there exists a (necessarily unique) complete set $e_1, \dots, e_n$ of orthogonal idempotents in $\End(\_RM)$ with $M_i = Me_i$.

\sepline

For a ring $R$, there are three natural ``regular'' modules to consider: $\_RR$, $R_R$, and $\_RR_R$.

Some results to be shown and expanded upon.

A left ideal $I$ of a ring $R$ is a direct summand of $\_RR$ if and only if there is an idempotent $e \in R$ such that $I = Re$.

If $e \in R$ is an idempotent, then so is $1 - e$, and $Re$ and $R(1 - e)$ are direct complements of each other, i.e., $\_RR = Re + R(1 - e)$.

Any direct decomposition of $\_RR$ has only finitely many nonzero summands.
(look at $\pi_\alpha(1)$)

Let $I_1, \dots, I_n$ be left ideals of $R$ (equiv $I_i \leq \_RR$?). TFAE:
\begin{itemize}
    \item $R = I_1 \oplus \cdots \oplus I_n$;
    \item Each $r \in R$ has a unique expression $r = r_1 + \cdots r_n$ with $r_i \in I_i$.
    \item There is a (necessarily unique) complete set of orthogonal idempotents $e_1, \dots, e_n \in R$ with $I_i = Re_i$.
\end{itemize}

If $e_1, \dots, e_n \in R$ is a complete set of orthogonal idempotents, then
\[
    \_RR = Re_1 \oplus \cdots \oplus Re_n
    \isp{and}
    R_R = e_1R \oplus \cdots \oplus e_nR.
\]

Let $e \in R$ be a nonzero idempotent. TFAE:
\begin{itemize}[nosep]
    \item $e$ is a primitive idempotent;
    \item $Re$ is a primitive left ideal of $R$;
    \item $eR$ is a primitive right ideal of $R$;
    \item $Re$ is an indecomposable direct summand of $\_RR$;
    \item $eR$ is an indecomposable direct summand of $R_R$;
    \item The ring $eRe$ has exactly one nonzero idempotent, namely $e$.
\end{itemize}

$\_RR = I_1 \oplus \cdots \oplus I_n$ with each $I_i$ a primitive left ideal of $R$ if and only if there exists a complete set of orthogonal primitive idempotents $e_1, \dots, e_n \in R$ such that $I_i = Re_i$.

\sepline

Something about ring direct sums.

\sepline


Let $\UU$ be a collection of modules.
A module $M$ is \keyword{(finitely) generated} by $\UU$ if there is a (finite) collection $\{U_\alpha\}_{\alpha \in A} \seq \UU$ and an epimorphism $\bigoplus_{\alpha \in A} U_\alpha \to M$.

In case $\UU = \{U\}$ is a singleton, say $U$ (finitely) generates $M$; which means there is an epimorphism $U^{(A)} \to M$ for some (finite) index set $A$.

If a module $\_RM$ has a spanning set $X \seq M$, then there is an epimorphism $R^{(X)} \to M$.
Moreover, $R$ finitely generates $M$ if and only if $M$ has a finite spanning set.

\sepline

A left $R$-module $T$ is simple if and only if $T \iso R/\mm$ for some maximal left ideal $\mm$ of $R$.


Suppose $\_RM = \sum_{\alpha \in A} T_\alpha$ with each $T_\alpha$ simple.
For every submodule $K \leq M$ there is a subset $B \seq A$ such that $M = K \oplus \bigoplus_{\beta \in B} T_\beta$.

Taking $N = 0$, we deduce that a module spanned by simple submodules is semisimple.

If $M$ is semisimple, then every short exact sequence $0 \to K \to M \to N \to 0$ splits.

A few more, but big conclusion is...

Let $M$ be a left $R$-module. TFAE:
\begin{itemize}[nosep]
    \item $M$ is semisimple;
    \item $M$ is generated by simple modules;
    \item $M$ is the sum of some simple submodules;
    \item $M$ is the sum of all its simple submodules;
    \item every submodule of $M$ is a direct summand;
    \item every short exact sequence $0 \to K \to M \to N \to 0$ splits.
\end{itemize}

\sepline

All semisimple modules, noetherian modules, and artinian modules have indecomposable decompositions.

However, not all modules admit indecomposable decompositions.
For example, consider the ring of continuos functions $R = C^0(\Q, \R)$, then the left regular modules $\_RR$ has no indecomposable direct summands.

\sepline

Let $M$ be a module.

Two direct decompositions
\[
    M = \bigoplus_{\alpha \in A} M_\alpha = \bigoplus_{\beta \in B} N_\beta
\]
are said to be \keyword{equivalent} if there is a bijection $\sigma : A \to B$, called an \keyword{equivalence map}, such that $M_\alpha \iso N_{\sigma(\alpha)}$.
It is easy to check that, as one should expect, this is indeed an equivalence relation.

Suppose $M = \bigoplus_{\alpha \in A} M_\alpha = \bigoplus_{\beta \in B} N_\beta$ and a function $\sigma : A \to B$.
Then these two decompositions are equivalent if and only if there is an automorphism $f$ such that $f(M_\alpha) = N_{\sigma(\alpha)}$.

It is in general not the case that any two indecomposable decompositions of a given modules are equivalent.
For example, let $I$ and $J$ be two left ideals of a ring $R$ such that $R = I + J$, then $I \oplus J \iso R \oplus (I \cap J)$ as left $R$-modules.

\sepline

A decomposition $M = \bigoplus_{\alpha \in A} M_\alpha$ is said to \keyword{complement (maximal) direct summands} if for every (maximal) direct summand $K$ of $M$ there is a subset $B \seq A$ such that
\[\textstyle
    M = \left(\bigoplus_{\beta \in B} M_\beta \right)\oplus K.
\]
This is a generalization of a property of semisimple modules.

A decomposition which complements all direct summands is necessarily indecomposable.

Suppose $M = \bigoplus_{\alpha \in A} M_\alpha$ is a decomposition that complements (maximal) direct summands.
If $f : M \to M'$ is an isomorphism, then
\[
    M' = \bigoplus_{\alpha \in A} f(M_\alpha)
\]
is a decomposition that complements (maximal) direct summands.

In particular, if a decomposition of $M$ complements (maximal) direct summands, then so does any equivalent decomposition.

\sepline

Lemma:

Let $M = \bigoplus_{\alpha \in A} M_\alpha$ be a decomposition that complements maximal direct summands.
If we have another decomposition
\[
    M = N_1 \oplus \cdots \oplus N_n \oplus K
\]
with each $N_i$ indecomposable, then there exist $\alpha_i \in A$ such that $M_{\alpha_i} \iso N_i$ and for $\ell = 1, \dots n$,
\[
    M = M_{\alpha_1} \oplus \cdots \oplus M_{\alpha_\ell} \oplus N_{\ell + 1} \oplus \cdots \oplus N_n \oplus K.
\]

Lemma:

Let $M = \bigoplus_{\alpha \in A} M_\alpha$ be a decomposition that complements (maximal) direct summands.
For any subset $B \seq A$, $N = \bigoplus_{\beta \in B} N_\beta$ is a decomposition that complements (maximal) direct summands.

Moreover, if $M$ has a decomposition that complements direct summands, then so does every direct summand of $M$.

Anderson \& Fuller (1992): ``\textit{Incidentally, it is apparently not known whether the last assertion holds...for decompositions just complementing maximal direct summands.}''
(should check if this has been resolved.)

Theorem:

If a module $M$ has an indecomposable decomposition that complements maximal direct summands, then all indecomposable decompositions of $M$ are equivalent.

Corollary:

If a module $M$ has an indecomposable decomposition that complements (maximal) direct summands, then every indecomposable decomposition of $M$ complements (maximal) direct summands.

\sepline

If $R$ is a local ring, then $0$ and $1$ are its only idempotents.

Azumaya Theorem:

Let $M = \bigoplus_{\alpha \in A} M_\alpha$ be a decomposition.
If each endomorphism ring $\End(M_\alpha)$ is local, then this is an indecomposable decomposition and
\begin{itemize}
    \item every nonzero direct summand of $M$ has an indecomposable direct summand;
    \item the decomposition $M = \bigoplus_{\alpha \in A} M_\alpha$ complements maximal direct summands and this is equivalent to every indecomposable decomposition of $M$.
\end{itemize} 

Corollary:

Let $M = M_1 \oplus \cdots \oplus M_n$ be a finite decomposition.
If each $\End(M_i)$ is local, then the decomposition complements direct summands.

\sepline

Lemma:

If $M$ is an indecomposable module of finite length, then $\End(M)$ is a local ring.

Krull-Schmidt Theorem:

Let $M$ be a nonzero module of finite length.
Then $M$ has a finite indecomposable decomposition
\[
    M = M_1 \oplus \cdots \oplus M_n
\]
such that for every other indecomposable decomposition
\[
    M = N_1 \oplus \cdots \oplus N_k,
\]
$n = k$ and there is a permutation $\sigma$ such that $M_{\sigma(i)} \iso N_i$ and for $\ell = 1, \dots, n$,
\[
    M = M_{\sigma(1)} \oplus \cdots \oplus M_{\sigma(\ell)} \oplus N_{\ell + 1} \oplus \cdots \oplus N_n.
\]
In fact, the decomposition $M = M_1 \oplus \cdots \oplus M_n$ complements direct summands.

\sepline

Let $D$ be a division ring (nonzero inverses/noncommutative field) and $n \in \N$.

Let
\[
    C_n(D) = M_{n \times 1}(D) = \mat{D \\ \vdots \\ D}
    \isp{and}
    R_n(D) = M_{1 \times n}(D) = \mat{D & \cdots & D} 
\]
denote the sets of $n \times 1$ column and $1 \times n$ row matrices with entries in $D$.
Considering these as right and left $D$-vector spaces,
\[
    C_n(D) = (D_D)^n \isp{and} R_n(D) = (\_DD)^n.
\] 
Moreover, the usual matrix multiplications are ring isomorphisms
\[
    \lambda : M_n(D) \To \End(C_n(D)_D) \isp{and} \ell : M_n(D) \To \End(\_DR_n(D)).
\]
So $C_n(D)$ and $R_n(D)$ are left and right $M_n(D)$-modules respectively.

In fact, both are simple $M_n(D)$-modules; let $E_1, \dots, E_n$ be the primitive diagonal idempotents of $M_n(D)$.
Then as a left $M_n(D)$-module
\[
    M_n(D)
        = M_n(D)E_1 \oplus \cdots \oplus M_n(D)E_n
        \iso C_n(D) \oplus \cdots \oplus C_n(D)
\]
and as a right $M_n(D)$-module
\[
    M_n(D)
        = E_1M_n(D) \oplus \cdots \oplus E_nM_n(D)
        \iso R_n(D) \oplus \cdots \oplus R_n(D).
\]
In particular, both left and right regular modules of $M_n(D)$ are generated by a simple module.
By some result, every left $M_n(D)$-module is generated by the simple $M_n(D)$-module $C_n(D)$ and every right $M_n(D)$-module is generated by the simple $M_n(D)$-module $R_n(D)$.

\sepline

Let $R$ be a ring, $M$ a nonzero left $R$-module, and $n \in \N$.
Write endomorphisms of both $M$ and $M^{(n)} = M^{\oplus n}$ as right operators, and we also write the natural inclusions and projections
\[
    \iota_i : M \to M^{(n)} \isp{and} \pi_i : M^{(n)} \to M
\]
on the right.
For each $\alpha = [\alpha_{ij}] \in M_n(\End(M))$ define $\rho(\alpha) \in \End(M^{(n)})$ componentwise by
\[
    (x\rho(\alpha))\pi_j = \sum_{i=1}^{n} x \pi_i \alpha_{ij}.
\]
Then $x\rho(\alpha)$ is simply the usual matrix product
\[
    x\rho(\alpha) = \mat{x_1 & \cdots & x_n}[\alpha_{ij}]
\]
where the elements $x$ of $M^{(n)}$ are considered as row matrices $x = \mat{x_1 & \cdots & x_n}$ over $M$.
By the properties of ordinary matrix multiplication, we deduce that $M^{(n)}$ is a bimodule
\[
    \_RM^{(n)}{}_{M_n(\End(M))}
\]
via $\rho$, i.e.,
\[
    \rho : M_n(\End(M)) \To \End(M^{(n)})
\]
is a ring homomorphism.
In fact, one can check that this is an isomorphism.

\sepline

Schur Lemma:

If $\_RT$ is a simple module, then $\End(\_RT)$ is a division ring.

Wedderburn Theorem:

The ring $R$ has as simple left generator if and only if $R$ is isomorphic to the full submatrix ring $M_n(D)$ for some division ring $D$ and some $n \in \N$.
Moreover, if $\_RT$ is a simple left generator for $R$, then as a ring $R \iso M_n(D)$ with $D = \End(\_RT)$ and $n = \mathrm{length}(\_RR)$ or
\[
    R \iso M_{\mathrm{length}(\_RR)}(\End(\_RT))
\]

\sepline

For a ring $R$ TFAE:
\begin{itemize}[nosep]
    \item $R$ has a simple left generator;
    \item $R$ is a simple and left artinian;
    \item For some simple $\_RT$, $\_RR \iso T^{(n)}$ for some $n$;
    \item $R$ is simple and $\_RR$ is semisimple.
\end{itemize}
Moreover, each of these is equivalent to the dual result (swap left and right).

In particular, for simple rings the conditions of left artinian, right artinian, and artinian are equivalent.

A ring satisfying these equivalent conditions is called a \keyword{simple artinian ring}.

\sepline

A ring $R$ is \keyword{semisimple} if the left regular module $\_RR$ is semisimple.

Every simple artinian ring is semisimple.

Any ring direct sum of semisimple rings is also semisimple.

Wedderburn-Artin Theorem:

A ring is semisimple if and only if it is a finite (ring) direct sum of simple artinian rings.

More Structure:

Let $R$ be a semisimple ring.
Then $R$ contains a finite set $T_1, \dots, T_m$ of minimal left ideals which comprise and irredundant set of representatives of the simple left $R$-modules.
Moreover, for each such set the homogeneous components
\[
    \mathrm{Tr}(T_i) = RT_iR
\]
are simple artinian rings and $R$ is the direct sum
\[
    R = RT_1R \dotplus \cdots \dotplus RT_mR.
\]
Finally, $T_i$ is a simple generator for the ring $RT_iR$ and
\[
    RT_iR \iso M_{n_i}(D_i)
\]
where $n_i = \mathrm{length}(RT_iR)$ and $D_i = \End(\_RT_i)$.

\sepline

For a ring $R$ TFAE:
\begin{itemize}[nosep]
    \item $R$ is semisimple;
    \item $R$ has a semisimple left generator;
    \item Every short exact sequence $0 \to K \to M \to N \to 0$ of left $R$-modules splits
    
    (equiv to every mono/epi in $R\text{-}\mathsf{Mod}$ splitting);

    \item Every left $R$-module is semisimple;
\end{itemize}

\sepline

Maschke Theorem:

If $G$ is a group of order $n$ and $K$ is a field whose characteristic does not divide $n$, then the group ring $KG$ is semisimple.

Theorem:

If $R$ be a simple ring of $m$ elements, then
\begin{itemize}
    \item $Z(R) = \F_{p^n}$ for some prime $p$ and some $n$;
    \item $m = (p^n)^{k^2}$ for some $k$;
    \item $R \iso M_k(\F_{p^n})$.
\end{itemize}

As a corollary, there is a natural bijection between the finite semisimple rings (up to isomorphism) and the set of all finite sequences (ordered? surely not)
\[
    (p_1, n_1, k_1), \dots, (p_\ell, n_\ell, k_\ell)
\]
of triples of natural numbers with each $p_i$ prime.
\[
    \{\text{finite semisimple rings}\}/\text{iso} \longleftrightarrow \bigsqcup_{\ell \in \N} (\N_{\mathrm{prime}} \times \N \times \N)^\ell
\]

\sepline

If $D$ is a division ring and $M_D$ is a right $D$-vector space, then $M_D$ is free, so the ring $\End(M_D)$ of endomorphisms of $M_D$ is isomorphic to the ring of column finite matrices over $D$.
That is, if $\{x_i\}_{i \in I}$ is a basis for $M_D$ then the mapping
\begin{align*}
    \End(M_D) &\To \mathrm{CFM}_I(D) \\
    a &\Mapsto [a_{ij}]
\end{align*}
defined by
\[
    a(x_j) = \sum_{i \in I} x_i a_{ij} \isp{and} a_{ij} = \pi_i(a(x_j)).
\]







\end{document}