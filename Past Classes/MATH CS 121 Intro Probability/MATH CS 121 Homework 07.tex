\documentclass[12pt]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb}

% Problem Box
\setlength{\fboxsep}{4pt}
\newsavebox{\mybox}
\newenvironment{problem}
    {\begin{lrbox}{\mybox}\begin{minipage}{0.98\textwidth}}
    {\end{minipage}\end{lrbox}\framebox[\textwidth]{\usebox{\mybox}}}

% Options
\renewcommand{\thesubsection}{\thesection(\alph{subsection})}
\allowdisplaybreaks
\addtolength{\jot}{1em}

% Default Commands
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newcommand{\ds}{\displaystyle}
\newcommand{\isp}[1]{\quad\text{#1}\quad}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\emptyset}{\varnothing}

% Extra Commands
\renewcommand{\P}{\mathbb{P}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\Poi}{\operatorname{Poi}}

\begin{document}
 
\title{Homework 7\\
    \large MATH CS 121 Intro to Probability
}
\author{Harry Coleman}
\date{December 18, 2020}
\maketitle

\section{}

Our sample space $\Omega$ is the set of possible graphs on three vertices. Let the set $\{1, 2, 3\}$ denote the three vertices and let $E = \{e_{12}, e_{23}, e_{13}\}$ denote the set of possible edges. Then for a graph $G \in \Omega$, we say that an edge $e_{ij} \in G$ if there exists an edge in $G$ connecting vertices $i$ and $j$. Then we consider our sample space to be the power set of edges, i.e., $\Omega = 2^E$. With a finite sample space, we take the power set $\sigma$-algebra $\FF = 2^\Omega$.

Then the random variables $X_1, X_2$ can be defined by
\[
    X_1(G) = I_G(e_{12}) + I_G(e_{13}) \isp{and} X_2(G) = I_G(e_{12}) + I_G(e_{23}),
\]
where $I_G$ is the indicator function,
\[
    I_G(e) = \begin{cases}
        1 &\text{if $e \in G$,} \\
        0 &\text{otherwise.}
    \end{cases}
\]
Moreover, we will enumerate a graph $G \in \Omega$ by
\[
    G = I_G(e_{12})\, I_G(e_{23})\, I_G(e_{13}).
\]
For example, the graph $011$ does not have an edge connecting vertices $1$ and $2$, but does have an edge connecting vertices $2$ and $3$, and an edge connecting vertices $1$ and $3$. And the graph $000$ has no edges.

\subsection{}

Evidently, $X_1$ and $X_2$ are functions from $\Omega$ to $\R$, in particular to the subset $\{0, 1, 2\}$. Since the $\sigma$-algebra is the power set of $\Omega$, then any subset of $\Omega$ is in $\FF$. In particular, for any borel subset of $\R$, the preimage under $X_1$ or $X_2$ is a subset of $\Omega$, and therefore in $\FF$. Thus, $X_1$ and $X_2$ are measurable functions, i.e., random variables.

\subsection{}

As noted in 1(a), the range of both $X_1$ and $X_2$ is the set $\{0, 1, 2\}$. We first find $\sigma(X_1)$. The set of preimages under $X_1$ is
\[
    \{X_1^{-1}(0), X_1^{-1}(1), X_1^{-1}(2)\} = \{\{000, 010\}, \{100, 110, 001, 011\}, \{101, 111\}\}.
\]
For each $i \in \{0, 1, 2\}$, we denote $A_i = X_1^{-1}(i)$. So the above can be written as $\{A_0, A_1, A_2\}$. Then the closure of this set under complement and countable union gives us
\[
    \sigma(X_1) = \{\emptyset, A_0, A_1, A_2, (A_1 \cup A_2), (A_2 \cup A_3), (A_1 \cup A_3), \Omega\}.
\]
Similarly, for $X_2$, we let $B_i = X_2^{-1}(i)$ for each $i \in \{0, 1, 2\}$. Then we obtain the sigma algebra generated by $X_2$,
\[
    \sigma(X_2) = \{\emptyset, B_0, B_1, B_2, (B_1 \cup B_2), (B_2 \cup B_3), (B_1 \cup B_3), \Omega\},
\]
where
\[
    \{B_0, B_1, B_2\} = \{\{000, 001\}, \{100, 101, 010, 011\}, \{110, 111\}\}.
\]

\subsection{}

The probability of a $G \in \Omega$ is $p$ raised to the power of the number of edges in $G$. in other words, if $G$ is enumerated by $ijk$, then the probability is given by
\[
    \P(G) = p^{i + j + k}(1 - p)^{3 - (i + j + k)}.
\]
We consider the events that $X_1 = 0$ and $X_2 = 2$. Using the preimage sets defined in 1(b), we find
\[
    \P(X_1 = 0) = \P(A_0) = \P(000) + \P(001) = p^0(1-p)^3 + p^1(1-p)^2 = (1 - p)^2.
\]
Intuitively, the probability that $X_1 = 0$ is the probability that vertex $1$ has no adjacent edges. Since the vertex $1$ has two possible adjacent edges, the probability of this would be the probability that two particular edges are not in a graph, which agrees with the above. Similarly, we find
\[
    \P(X_2 = 2) = \P(B_2) = \P(110) + \P(111) = p^2(1-p)^1 + p^3(1-p)^0 = p^2.
\]
However, considering the probability that both of these events occurring, we find
\[
    \P(X_1 = 0, X_2 = 2) = \P(A_0 \cap B_2) = \P(\emptyset) = 0.
\]
Intuitively, $X_1 = 0$ requires that two particular edges are not present and $X_2 = 2$ requires that 2 particular edges are present. Since there are only $3$ total edges possible, these two events could not occur simultaneously. More specifically, $X_1 = 0$ requires the edge $e_{12}$ not to be present, but $X_2 = 2$ requires that same edge to be present.

\subsection{}

We have $X_1, X_2 \sim \Bin(2, p)$. The probability that one of them is equal to $k$ is given by the number of ways to have $k$ edges present out of two possible adjacencies, times the probability of $k$ particular edges occurring. 

\newpage
\section{}

\subsection{}

\begin{proof}
    Let $\eps > 0$ be given. Since $\E[X_n] \to 0$, then we let $N \in \N$ such that
    \[
        n \geq N \implies \E[X_n] < \eps.
    \]
    Then Markov's inequality gives us
    \[
        \P(X_n \geq 1) \leq \frac{\E[X_n]}{1} < \eps.
    \]
    Thus, we have $\P(X_n \geq 0) \to 0$. And since
    \[
        \P(X_n = 0) = 1 - \P(X_n \geq 1),
    \]
    then we have $\P(X_n = 0) \to 1$.
    
\end{proof}


\subsection{}

First, we find
\[
    m_X'(t) = 3(1 - 2t)^{-5/2)}.
\]
Then Markov's inequality gives us
\[
    \P(X \geq 8) = \frac{\E[X]}{8} = \frac{m'_X(0)}{8} = \frac{3}{8}.
\]
Next, we find
\[
    m_X''(t) = 15(1 - t)^{-7/2}.
\]
Then Chebyshev's inequality gives us
\[
    \P(X \geq 8) = \P(X - E[X] \geq 5) \leq \frac{\Var(X)}{5^2} = \frac{m_X''(0) - \E[X]^2}{25} = \frac{6}{25}.
\]

\newpage
\section{}

\subsection{}

First we find
\[
    m_X'(t) = \frac{-4}{\pi}e^{-4t} + \frac{4}{\pi}e^{4t}.
\]
So the expected value is
\[
    \E[X] = m_X'(0) = \frac{-4}{\pi}e^{-4(0)} + \frac{4}{\pi}e^{4(0)} = 0.
\]

\subsection{}

The moment generating function of $X$ is given by
\[
    m_X(t) = \E[e^{tX}] = \sum_{k} e^{tk} \P(e^{tX} = e^{tk}) = \sum_{k} e^{tk} \P(tX = tk).
\]
For $t = 0$, we have
\[
    m_X(0) = \E[e^{0X}] = \E[1] = 1.
\]
Then if $t \ne 0$, we can take the
\[
    m_X(t) = \E[e^{tX}] = \sum_{k} e^{tk} \P(X = k).
\]
This implies that the constant term of $m_X(t)$ is for $k = 0$, since every other term has a factor of $e^{tk}$. In particular, the constant term of $m_X(t)$ is
\[
    e^{t0} \P(X = 0) = \P(X = 0)
\]
Since we are given
\[
    m_X(t) = \frac{1}{\pi}e^{-4t} + \frac{1}{\pi}e^{4t} + \left( 1 - \frac{2}{\pi} \right),
\]
then we have
\[
    \P(X = 0) = 1 - \frac{2}{\pi}.
\]
Moreover, for $k = 4$ and $k = -4$ we obtain the terms of $m_X(t)$ with factors of $e^{4t}$ and $e^{-4t}$, respectively. Thus, we have
\[
    \P(X = 4) = \P(X = -4) = \frac{1}{\pi}.
\]
Now since
\[
    \sum_{k = 0, 4, -4} \P(X = k) = 1 - \frac{2}{\pi} + \frac{1}{\pi} + \frac{1}{\pi} = 1,
\]
then the probability mass function is zero on all other values of $k$.


\section{}

\subsection{}

Because $X_n$ is a Poisson distribution with argument $n$, then it has the same distribution as the sum of $n$ copies of a Poisson distribution with argument $1$, i.e., $X_1$. In other words, $X_n$ is the sum of $n$ independent and identically distributed random variables with mean $1$. Then the strong law of large numbers tells us that
\[
    \P \left( \lim_{n \to \infty} \frac{X_n}{n} = 1 \right) = 1.
\]

\subsection{}

\begin{proof}
    Since $X_n = \sum_{k=1}^n X_1$, then it is the sum of $n$ independent and identically distributed random variables. Since $\E[X_1] = 1$ and $\Var(X_1) = 1$, then the central limit theorem gives us
    \[
        \lim_{n \to \infty} \P \left( \frac{X_n - n}{\sqrt{n}} \leq x \right) 
            = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \,dt.
    \]
    If $X \sim N(0,1)$, i.e., a standard normal distribution, then the probability density function of $X$ is given by
    \[
        \frac{1}{\sqrt{2\pi}} e^{-t^2/2}.
    \]
    Thus, we have
    \[
         \lim_{n \to \infty} \P \left( \frac{X_n - n}{\sqrt{n}} \leq x \right) 
            = \P(X \leq x).
    \]
    
\end{proof}

\subsection{}

\begin{proof}
    For some $n \in \N$, we consider
    \[
        \P \left( \frac{X_n - n}{\sqrt{n}} \leq 0 \right) = \P(X_n \leq n).
    \]
    Since $X_n$ is a Poisson random variable with argument $n$, then we have
    \[
        \P(X_n \leq n) 
            = \sum_{k = 0}^n \P(X_n = k) 
            = \sum_{k = 0}^n e^{-n} \frac{n^k}{n!} 
            = e^{-n} \sum_{k = 0}^n \frac{n^k}{n!}
    \]
    Since the standard normal distribution has an expected value of $0$ and it is symmetric, then we have
    \[
        \P(X \leq 0) = \frac12.
    \]
    Then applying the result of 4(b) to $x = 0$ and substituting the above values, we obtain
    \[
        \lim_{n \to \infty} e^{-n} \sum_{k = 0}^n \frac{n^k}{n!} =  \frac12.
    \]
    
\end{proof}

\newpage
\section{}

\begin{proof}
    The random variables $X_1, X_2, \dots$ are independent and identically distributed. In particular, they are Poisson random variables with argument $\lambda$, implying that $\E[X_n] = \lambda$ for all $n \in \N$. Applying the weak law of large numbers, we obtain
    \[
        \lim_{n \to \infty} \P\left( \left| \frac{S_n}{n} - \lambda \right| < |t - \lambda| \right) = 1.
    \]
    If $\lambda < t$, then we have
    \begin{align*}
        \P\left( \left| \frac{S_n}{n} - \lambda \right| < |t - \lambda| \right)
        &= \P\left( \left| \frac{S_n}{n} - \lambda \right| < t - \lambda \right) \\
            &= \P\left( \lambda - t < \frac{S_n}{n} - \lambda < t - \lambda \right) \\
            &= \P\left( 2\lambda - t < \frac{S_n}{n} < t \right) \\
            &\leq \P\left( \frac{S_n}{n} \leq t \right).
    \end{align*}
    Then since the probability measure is at most $1$, then we have the limit
    \[
        \lim_{n \to \infty} P\left( \frac{S_n}{n}\leq t \right) = 1.
    \]
    Now if $\lambda > t$, then we have
    \begin{align*}
        \P\left( \left| \frac{S_n}{n} - \lambda \right| < |t - \lambda| \right)
        &= \P\left( \left| \frac{S_n}{n} - \lambda \right| < \lambda - t \right) \\
            &= \P\left( t - \lambda < \frac{S_n}{n} - \lambda < \lambda - t \right) \\
            &= \P\left( t < \frac{S_n}{n} < 2\lambda - t \right) \\
            &\leq \P\left( t < \frac{S_n}{n}\right).
    \end{align*}
    And we have the limit
    \[
        \lim_{n \to \infty} P\left( \frac{S_n}{n}\leq t \right) = 1 - \lim_{n \to \infty} \P\left( t < \frac{S_n}{n}\right) = 1 - 1 = 0.
    \]
    
\end{proof}


\section{}

\subsection{}

Let $X$ be the random variable representing the lifetime of the bulb. Then the cumulative distribution function of $X$, i.e., the probability that the light bulb burns out before $T$ hours, is given by.
\[
    \P(X \leq T) = 1 - e^{-\lambda T}.
\]
Therefore, the probability that the light bulb will not burn out after $T$ hours is
\[
    \P(X \geq T) = 1 - \P(X \leq T) = 1 - 1 + e^{-\lambda T} = e^{-\lambda T}.
\]
We are given that the expected value of $X$ is $100$. Since $X$ has an exponential distributions,
\[
    \E[X] = \frac1\lambda,
\]
and we have $\lambda = 1/100$. Thus,  the probability that the light bulb will not burn out after $T$ hours is
\[
    e^{-T/100}.
\]

\subsection{}

Setting $e^{-T/100}$ to $1/2$, we find
\begin{align*}
    e^{-T/100} &= \frac12, \\
    \frac{-T}{100} &= \log\frac12, \\
    T &= -100 \log \frac12 \approx 69.3147.
\end{align*}


\newpage
\section{}

\subsection{}

The random variable $X$ has a binomial distribution over $48000$ trials with a probability of success $1/6$. A trial is a roll of a die and a success is that die landing on $6$. That is, $X \sim \Bin(48000, 1/6)$.

\subsection{}

Since $X$ has a binomial distribution, then
\[
    \P(7500 \leq X \leq 8500) 
        = \sum_{k = 7500}^{8500} \P(X = k) 
        = \sum_{k = 7500}^{8500} \binom{48000}{k} \left( \frac16 \right)^k \left( \frac56 \right)^{48000-k}. 
\]

\subsection{}

By the central limit theorem for binomial distributions, we have
\[
    \lim_{n \to \infty} \P \left( a \leq \frac{S_n - np}{\sqrt{np(1-p)}} \leq b \right)
        = \Phi(b) = \Phi(a),
\]
where $S_n \sim \Bin(n,p)$. In other words, for a large number of trials, a binomial distribution can be approximated by a standard normal distribution. Assuming $48000$ is sufficiently large for our purposes, we let
\[
    a = \frac{7500 - np}{\sqrt{np(1-p)}} 
        = \frac{7500 - 48000 \cdot \frac16}{\sqrt{48000 \cdot \frac16 \cdot \frac56}} 
        \approx -6.1237
\]
and
\[
    b = \frac{8500 - np}{\sqrt{np(1-p)}} 
        = \frac{8500 - 48000 \cdot \frac16}{\sqrt{48000 \cdot \frac16 \cdot \frac56}} 
        \approx 6.1237.
\]
Then we have
\[
    \P(7500 \leq X \leq 8500)
        \approx \Phi(b) - \Phi(a)
        \approx \frac{1}{\sqrt{2\pi}}\int_{-6.1237}^{6.1237} e^{-t^2/2} \,dt 
        \approx 1.
\]


\newpage
\section{}

Given that $X \sim N(10, 36)$, then we have $(X - 10)/6 \sim N(0,1)$. Using this fact, we obtain the following probabilities.

\subsection{}

\[
    \P(X > 5) = \P\left( \frac{X - 10}{6} > \frac{-5}{6} \right) = 1 - \Phi(\tfrac{-5}{6}).
\]

\subsection{}

\[
    \P(4 < X < 16) = \P\left( -1 < \frac{X - 10}{6} < 1 \right) = \Phi(1) - \Phi(-1).
\]

\subsection{}

\[
    \P(X < 8) = \P\left(\frac{X - 10}{6} < \frac{-1}{3} \right) = \Phi(\tfrac{-1}{3}).
\]

\end{document}